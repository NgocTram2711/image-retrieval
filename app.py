# app.py
import streamlit as st
from PIL import Image
import numpy as np
import pickle
import json
from sentence_transformers import SentenceTransformer, util

# Import c√°c class extractor t·ª´ file feature_extractor.py
from feature_extractor import FeatureExtractorClip, FeatureExtractorSiglip2, FeatureExtractorBlip2

# --- C·∫•u h√¨nh trang ---
st.set_page_config(page_title="H·ªá th·ªëng Truy v·∫•n H√¨nh ·∫£nh", layout="wide")

# --- D·ªØ li·ªáu hi·ªáu nƒÉng c·ªßa c√°c model ---
PERFORMANCE_METRICS = {
    "CLIP (Fine-tuned)": {
        "text_to_image": {"mAP": 0.221, "mRecall@10": 0.490},
        "image_to_image": {"mAP": 0.996, "mRecall@10": 1.000},
    },
    "SigLIP (Fine-tuned)": {
        "text_to_image": {"mAP": 0.002, "mRecall@10": 0.002},
        "image_to_image": {"mAP": 0.997, "mRecall@10": 1.000},
    },
    "BLIP-2 (Caption Search)": {
        "text_to_image": {"mAP": 0.326, "mRecall@10": 0.584},
        "image_to_image": {"mAP": 1.000, "mRecall@10": 1.000},
    }
}

# --- T·∫£i d·ªØ li·ªáu v√† model (s·ª≠ d·ª•ng cache c·ªßa Streamlit) ---

@st.cache_resource
def load_all_models():
    """T·∫£i t·∫•t c·∫£ c√°c model c·∫ßn thi·∫øt m·ªôt l·∫ßn duy nh·∫•t."""
    models = {
        "CLIP (Fine-tuned)": FeatureExtractorClip(),
        "SigLIP (Fine-tuned)": FeatureExtractorSiglip2(),
        "BLIP-2 (Caption Search)": FeatureExtractorBlip2(),
        "SentenceTransformer": SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')
    }
    return models

@st.cache_data
def load_indexed_data(model_name):
    """T·∫£i d·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c l·∫≠p ch·ªâ m·ª•c cho m·ªôt model c·ª• th·ªÉ."""
    if "CLIP" in model_name:
        file_path = "features_clip.pkl"
    elif "SigLIP" in model_name:
        file_path = "features_siglip2.pkl"
    else: # BLIP-2
        file_path = "features_blip2.pkl"
    
    try:
        with open(file_path, "rb") as f:
            data = pickle.load(f)
        return data["paths"], data["features"]
    except FileNotFoundError:
        st.error(f"L·ªñI: Kh√¥ng t√¨m th·∫•y file features '{file_path}'. Vui l√≤ng ch·∫°y 'index.py --model ...' cho model n√†y.")
        return None, None

@st.cache_data
def load_and_encode_captions(_st_model):
    """T·∫£i v√† m√£ h√≥a captions c·ªßa database cho BLIP-2."""
    try:
        with open("db_captions.json", "r", encoding='utf-8') as f:
            db_captions_data = json.load(f)
        
        # S·∫Øp x·∫øp captions theo ƒë√∫ng th·ª© t·ª± c·ªßa image paths
        img_paths, _ = load_indexed_data("BLIP-2 (Caption Search)")
        path_to_caption = {item['image_path']: item['caption'] for item in db_captions_data}
        ordered_captions = [path_to_caption.get(p, "") for p in img_paths]

        embeddings = _st_model.encode(ordered_captions, convert_to_tensor=True, show_progress_bar=True)
        return ordered_captions, embeddings
    except FileNotFoundError:
        st.error("L·ªñI: Kh√¥ng t√¨m th·∫•y file 'db_captions.json'. Vui l√≤ng ch·∫°y 'generate_database_captions.py' tr∆∞·ªõc.")
        return None, None

# T·∫£i t·∫•t c·∫£ model khi kh·ªüi ƒë·ªông
models = load_all_models()

# --- Giao di·ªán ng∆∞·ªùi d√πng ---

# Sidebar ƒë·ªÉ ch·ªçn model
st.sidebar.title("‚öôÔ∏è T√πy ch·ªçn")
selected_model_name = st.sidebar.selectbox(
    "Ch·ªçn m√¥ h√¨nh ƒë·ªÉ t√¨m ki·∫øm:",
    list(PERFORMANCE_METRICS.keys())
)

# Hi·ªÉn th·ªã hi·ªáu nƒÉng c·ªßa model ƒë√£ ch·ªçn
st.sidebar.header("Hi·ªáu nƒÉng Model")
metrics = PERFORMANCE_METRICS[selected_model_name]
st.sidebar.markdown("**Text-to-Image:**")
st.sidebar.text(f"  - mAP: {metrics['text_to_image']['mAP']:.3f}")
st.sidebar.text(f"  - mRecall@10: {metrics['text_to_image']['mRecall@10']:.3f}")
st.sidebar.markdown("**Image-to-Image:**")
st.sidebar.text(f"  - mAP: {metrics['image_to_image']['mAP']:.3f}")
st.sidebar.text(f"  - mRecall@10: {metrics['image_to_image']['mRecall@10']:.3f}")


# Ti√™u ƒë·ªÅ ch√≠nh
st.title("H·ªá th·ªëng Truy v·∫•n H√¨nh ·∫£nh ƒêa ph∆∞∆°ng ti·ªán üñºÔ∏èüìù")
st.write(f"Hi·ªán ƒëang s·ª≠ d·ª•ng model: **{selected_model_name}**")

# T·∫£i d·ªØ li·ªáu t∆∞∆°ng ·ª©ng v·ªõi model ƒë√£ ch·ªçn
img_paths, features_array = load_indexed_data(selected_model_name)

# N·∫øu l√† BLIP-2, t·∫£i th√™m d·ªØ li·ªáu caption
db_captions, caption_embeddings = (None, None)
if "BLIP-2" in selected_model_name:
    db_captions, caption_embeddings = load_and_encode_captions(models["SentenceTransformer"])

# T·∫°o 2 tab cho 2 lo·∫°i query
tab1, tab2 = st.tabs(["üîç T√¨m ki·∫øm b·∫±ng H√¨nh ·∫£nh", "üìù T√¨m ki·∫øm b·∫±ng VƒÉn b·∫£n"])

# --- Tab 1: T√¨m ki·∫øm b·∫±ng H√¨nh ·∫£nh ---
with tab1:
    st.header("T·∫£i l√™n m·ªôt ·∫£nh ƒë·ªÉ t√¨m ki·∫øm")
    uploaded_file = st.file_uploader("Ch·ªçn m·ªôt t·ªáp ·∫£nh", type=["jpg", "jpeg", "png"], key="image_uploader")

    if uploaded_file and img_paths:
        query_image = Image.open(uploaded_file)
        st.image(query_image, caption="·∫¢nh truy v·∫•n", width=200)

        if st.button("T√¨m ki·∫øm ·∫£nh t∆∞∆°ng t·ª±", key="search_image"):
            with st.spinner("ƒêang tr√≠ch xu·∫•t ƒë·∫∑c tr∆∞ng v√† t√¨m ki·∫øm..."):
                fe = models[selected_model_name]
                query_features = fe.extract_image_features(query_image)
                
                # Logic t√¨m ki·∫øm Image-to-Image l√† gi·ªëng nhau cho c·∫£ 3 model
                similarities = np.dot(features_array, query_features)
                result_indices = np.argsort(-similarities)[:12]

                st.header("K·∫øt qu·∫£ t√¨m ki·∫øm:")
                cols = st.columns(6)
                for i, idx in enumerate(result_indices):
                    with cols[i % 6]:
                        st.image(img_paths[idx], use_column_width=True)

# --- Tab 2: T√¨m ki·∫øm b·∫±ng VƒÉn b·∫£n ---
with tab2:
    st.header("Nh·∫≠p m√¥ t·∫£ vƒÉn b·∫£n ƒë·ªÉ t√¨m ki·∫øm")
    query_text = st.text_input("V√≠ d·ª•: 'a red car on the street', 'two cats sitting on a sofa'")

    if st.button("T√¨m ki·∫øm b·∫±ng vƒÉn b·∫£n", key="search_text"):
        if query_text and img_paths:
            with st.spinner("ƒêang m√£ h√≥a vƒÉn b·∫£n v√† t√¨m ki·∫øm..."):
                
                # --- LOGIC T√åM KI·∫æM PH√ÇN NH√ÅNH ---
                if "BLIP-2" in selected_model_name:
                    # Ph∆∞∆°ng ph√°p Text-vs-Text cho BLIP-2
                    if caption_embeddings is not None:
                        st_model = models["SentenceTransformer"]
                        query_embedding = st_model.encode(query_text, convert_to_tensor=True)
                        hits = util.semantic_search(query_embedding, caption_embeddings, top_k=12)[0]
                        result_indices = [hit['corpus_id'] for hit in hits]
                    else:
                        st.error("Kh√¥ng th·ªÉ t·∫£i d·ªØ li·ªáu caption cho BLIP-2.")
                        result_indices = []
                else:
                    # Ph∆∞∆°ng ph√°p truy·ªÅn th·ªëng cho CLIP v√† SigLIP
                    fe = models[selected_model_name]
                    query_features = fe.extract_text_features(query_text)
                    similarities = np.dot(features_array, query_features)
                    result_indices = np.argsort(-similarities)[:12]

                st.header("K·∫øt qu·∫£ t√¨m ki·∫øm:")
                cols = st.columns(6)
                for i, idx in enumerate(result_indices):
                    with cols[i % 6]:
                        st.image(img_paths[idx], use_column_width=True)
        else:
            st.warning("Vui l√≤ng nh·∫≠p m√¥ t·∫£ vƒÉn b·∫£n.")
