import json
from sentence_transformers import SentenceTransformer, util
import torch

# --- C·∫•u h√¨nh ---
CAPTIONS_DB_FILE = "db_captions.json"
# Model chuy√™n d·ª•ng ƒë·ªÉ so s√°nh s·ª± t∆∞∆°ng ƒë·ªìng ng·ªØ nghƒ©a gi·ªØa c√°c c√¢u
MODEL_NAME = 'sentence-transformers/all-MiniLM-L6-v2' 

print("B·∫Øt ƒë·∫ßu kh·ªüi t·∫°o h·ªá th·ªëng t√¨m ki·∫øm text-vs-text...")

# 1. T·∫£i model Sentence Transformer
device = "cuda" if torch.cuda.is_available() else "cpu"
try:
    model = SentenceTransformer(MODEL_NAME, device=device)
    print(f"‚úÖ Model '{MODEL_NAME}' ƒë√£ ƒë∆∞·ª£c t·∫£i tr√™n {device.upper()}.")
except Exception as e:
    print(f"L·ªñI: Kh√¥ng th·ªÉ t·∫£i model Sentence Transformer: {e}")
    exit(1)

# 2. T·∫£i v√† x·ª≠ l√Ω database captions
try:
    with open(CAPTIONS_DB_FILE, "r", encoding='utf-8') as f:
        database_captions = json.load(f)
    
    # T√°ch ri√™ng caption v√† ƒë∆∞·ªùng d·∫´n ·∫£nh ƒë·ªÉ x·ª≠ l√Ω
    corpus_captions = [item['caption'] for item in database_captions]
    corpus_image_paths = [item['image_path'] for item in database_captions]
    print(f"‚úÖ ƒê√£ t·∫£i {len(corpus_captions)} captions t·ª´ '{CAPTIONS_DB_FILE}'.")
except FileNotFoundError:
    print(f"L·ªñI: Kh√¥ng t√¨m th·∫•y file '{CAPTIONS_DB_FILE}'. Vui l√≤ng ch·∫°y script 'generate_database_captions.py' tr∆∞·ªõc.")
    exit(1)

# 3. M√£ h√≥a to√†n b·ªô corpus captions (ch·ªâ l√†m 1 l·∫ßn)
# ƒê√¢y l√† b∆∞·ªõc quan tr·ªçng ƒë·ªÉ t√¨m ki·∫øm di·ªÖn ra nhanh ch√≥ng
print("ƒêang m√£ h√≥a to√†n b·ªô captions trong database. Vi·ªác n√†y c√≥ th·ªÉ m·∫•t v√†i ph√∫t...")
corpus_embeddings = model.encode(corpus_captions, convert_to_tensor=True, show_progress_bar=True)
print("‚úÖ M√£ h√≥a ho√†n t·∫•t.")

def search(query: str, top_k: int = 8):
    """
    H√†m t√¨m ki·∫øm caption t∆∞∆°ng ƒë·ªìng nh·∫•t v·ªõi c√¢u truy v·∫•n.
    """
    print(f"\nüîç B·∫Øt ƒë·∫ßu t√¨m ki·∫øm cho truy v·∫•n: '{query}'")
    
    # M√£ h√≥a c√¢u truy v·∫•n
    query_embedding = model.encode(query, convert_to_tensor=True)
    
    # S·ª≠ d·ª•ng h√†m semantic_search c·ªßa th∆∞ vi·ªán ƒë·ªÉ t√¨m ki·∫øm hi·ªáu qu·∫£
    hits = util.semantic_search(query_embedding, corpus_embeddings, top_k=top_k)
    
    # hits l√† m·ªôt list c√°c k·∫øt qu·∫£, ta ch·ªâ l·∫•y k·∫øt qu·∫£ c·ªßa query ƒë·∫ßu ti√™n (v√† duy nh·∫•t)
    hits = hits[0] 
    
    results = []
    print("--- K·∫øt qu·∫£ t√¨m ki·∫øm ---")
    for hit in hits:
        # L·∫•y caption v√† ƒë∆∞·ªùng d·∫´n ·∫£nh t∆∞∆°ng ·ª©ng v·ªõi k·∫øt qu·∫£
        result_caption = corpus_captions[hit['corpus_id']]
        result_path = corpus_image_paths[hit['corpus_id']]
        score = hit['score']
        print(f"Score: {score:.4f} - Path: {result_path} - Caption: '{result_caption}'")
        results.append({
            "path": result_path,
            "score": score
        })
    return results

# --- Ch·∫°y th·ª≠ nghi·ªám ---
if __name__ == "__main__":
    # V√≠ d·ª• c√°c c√¢u truy v·∫•n
    search("two cats sitting on a sofa")
    search("a red car on the street")
    search("people playing on the beach")